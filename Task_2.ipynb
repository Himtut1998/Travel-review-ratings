{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Himtut1998/Travel-review-ratings/blob/main/Task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pdB2CtTVVdyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fefd134-2677-421c-db7f-7756e64bd699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score: 0.12619287568887538\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the documents from the Excel file\n",
        "input_file = 'combined_documents.xlsx'\n",
        "combined_df = pd.read_excel(input_file)\n",
        "\n",
        "# Lemmatization and stop words removal\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize by word\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Lemmatize, remove stop words and non-alphabetic tokens\n",
        "    lemmatized = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "# Preprocess documents\n",
        "combined_df['Processed_Documents'] = combined_df['Document'].apply(preprocess_text)\n",
        "\n",
        "# Convert the text documents into TF-IDF features\n",
        "vectorizer = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.7, ngram_range=(1,3))\n",
        "X = vectorizer.fit_transform(combined_df['Processed_Documents'])\n",
        "\n",
        "# Optionally, apply PCA for dimensionality reduction\n",
        "# pca = PCA(n_components=0.95)  # Adjust based on your dataset\n",
        "# X_reduced = pca.fit_transform(X.toarray())\n",
        "\n",
        "# Use K-means to cluster the documents\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Assign the cluster labels to the original dataframe\n",
        "combined_df['Cluster'] = kmeans.labels_\n",
        "\n",
        "# Function to assign a new document to a cluster\n",
        "def assign_to_cluster(new_document):\n",
        "    preprocessed_document = preprocess_text(new_document)\n",
        "    new_document_vector = vectorizer.transform([preprocessed_document])\n",
        "    cluster = kmeans.predict(new_document_vector)[0]\n",
        "    return cluster\n",
        "\n",
        "# Evaluate the performance using the silhouette score\n",
        "silhouette_avg = silhouette_score(X, kmeans.labels_)\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the function with a new document\n",
        "new_document = \"The local football team clinched the championship after a thrilling penalty shootout, marking their third national title in five years. Fans celebrated their victory across the city, highlighting the team's remarkable journey from underdogs to champions.\"\n",
        "predicted_cluster = assign_to_cluster(new_document)\n",
        "print(f\"Predicted cluster for the new document: {predicted_cluster}\")\n"
      ],
      "metadata": {
        "id": "eaVBk1lkXdl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70cd806f-9a8a-45b3-be48-743369b7c0b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted cluster for the new document: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zxMB6xUuWQev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GJjgZRL8X4PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DVHg_5-CYHKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "65YIT2Gkjv7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TjVNc189lEge"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}